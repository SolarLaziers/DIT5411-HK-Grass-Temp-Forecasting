DIT5411 — RNN & LSTM Grass Minimum Temperature Forecasting
==========================================================

1) Summary
----------
This project develops and compares two sequence models (SimpleRNN and stacked LSTM) to forecast Hong Kong daily minimum grass temperature. Data covers historical observations (raw CSV in data/) and a cleaned, interpolated version (data/daily_HKO_GMT_ALL_filled.csv). Processed numpy arrays and a scaler are stored in data/ (X_train.npy, y_train.npy, X_test.npy, y_test.npy, scaler.pkl). Model checkpoints are saved in models/ in Keras (.keras) format and plots in figures/.

Reported evaluation (from repository README)
- RNN:  MAE = 0.98 °C, RMSE = 1.33 °C
- LSTM: MAE = 0.95 °C, RMSE = 1.30 °C
Interpretation: LSTM shows a small but consistent improvement over the SimpleRNN baseline.

2) Data and preprocessing
-------------------------
- Raw CSV features bilingual header and special tokens ("***" for missing, trailing "C" unit).
- Preprocessing pipeline (notebooks/data_preprocessing.py) steps:
  - Header detection and flexible column mapping (Year/Month/Day/Value).
  - Clean Value: remove unit characters, convert '***' and blanks to NaN.
  - Build Date index; drop invalid dates.
  - Interpolate missing values using time-aware interpolation where possible, then forward/back-fill for edges.
  - Mark filled entries with WasFilled flag and save cleaned CSV: data/daily_HKO_GMT_ALL_filled.csv.
  - Split into train/test (default: train 1980–2024; test 2025-01-01–2025-10-30). If slices are empty, a last-10% fallback split is used.
  - Fit MinMaxScaler on training series only, transform train/test, create 30-day sliding-window sequences, and save arrays and scaler.

3) Models and training
----------------------
- RNN (notebooks/rnn_model.py)
  - Single SimpleRNN layer with 50 units, Dense(1) output.
  - Compiled with Adam (lr=0.001), MSE loss.
  - EarlyStopping and ModelCheckpoint used; models saved in Keras format (.keras).
- LSTM (notebooks/lstm_model.py)
  - Stacked LSTM: two LSTM layers (50 units), dropout between layers, Dense(1) output.
  - Same optimizer/loss/callback policy as RNN.
- Training uses validation_split (0.2) by default; consider time-series cross-validation (rolling origin) to avoid leakage.

4) Evaluation & diagnostics
---------------------------
- Predictions are made on scaled X_test, then inverse-transformed with scaler.pkl to return degrees Celsius.
- Metrics: MAE and RMSE reported. Visual checks included:
  - Actual vs Predicted time series plot.
  - Error distribution histograms for both RNN and LSTM.
- Interpretation guidelines:
  - MAE ~0.95–0.98 °C indicates sub-1°C average error — reasonable for daily min temperature forecasting.
  - Compare MAE and RMSE: small difference implies few large outliers; large RMSE >> MAE indicates occasional large errors.
  - Inspect WasFilled points to ensure interpolation did not leak future information into training/test sets.

5) How normalization is handled (recommended, exact steps)
----------------------------------------------------------
- Fit scaler on training data only (the raw un-windowed series).
  1. scaler = MinMaxScaler(feature_range=(0,1)); scaler.fit(train_values.reshape(-1,1))
  2. train_scaled = scaler.transform(train_values.reshape(-1,1))
     test_scaled  = scaler.transform(test_values.reshape(-1,1))
  3. Create sequences from the scaled arrays (sliding window of seq_length=30).
  4. Save scaler with joblib.dump(scaler, 'data/scaler.pkl') for later inverse-transform.
- In evaluation: inverse_transform both predictions and y_test using the saved scaler.

6) File map (key files)
-----------------------
- data/
  - daily_HKO_GMT_ALL.csv (raw)
  - daily_HKO_GMT_ALL_filled.csv (cleaned + WasFilled)
  - X_train.npy, y_train.npy, X_test.npy, y_test.npy, scaler.pkl
- notebooks/
  - data_preprocessing.py
  - rnn_model.py
  - lstm_model.py
  - evaluation.py
- models/
  - rnn_model.keras, rnn_model_final.keras
  - lstm_model.keras, lstm_model_final.keras
- figures/
  - rnn_training_loss.png, lstm_training_loss.png, predictions_plot.png, lstm_error_dist.png, rnn_error_dist.png
- report/
  - result.txt  (this file)

7) Recommendations & next steps
-------------------------------
- Use time-aware validation (walk-forward) instead of random validation_split to avoid temporal leakage.
- Add reproducibility: set numpy and tf seeds; log hyperparameters and training seeds.
- Evaluate sensitivity to sequence length and model capacity; try ensembling or residual correction models.
- Add exogenous features (calendar, humidity) to improve accuracy.
- Consider quantifying uncertainty (prediction intervals) via bootstrapping or Bayesian/ensemble methods.

8) Reproducibility commands (example)
-------------------------------------
# create venv and install
python -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
pip install -r requirements.txt

# preprocess
python notebooks/data_preprocessing.py

# train
python notebooks/rnn_model.py
python notebooks/lstm_model.py

# evaluate
python notebooks/evaluation.py

9) Conclusion
-------------
Models deliver sub-1°C MAE with the LSTM slightly better than the RNN. Proper train-only normalization and time-aware validation are crucial to ensure these results are reliable and generalize to future seasons.